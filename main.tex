\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Home assignment 2}
\author{Numerical Optimization and its Applications - Spring 2019\\Gil Ben Shalom, 301908877\\Tom Yaacov, 305578239}
\date{\today}

\usepackage[shortlabels]{enumitem}
\usepackage{pythontex}
\usepackage[most]{tcolorbox}
\usepackage{amsmath,amsthm,amssymb}

\newcommand{\importandtypeset}[1]{
  %\pyc{print(12);add_to_path('py_files'); from #1 import *; pytex.add_dependencies(os.path.join('py_files', '#1.py'))}%
  \inputpygments{python}{py_files/#1.py}%
  \pyc{from py_files import #1}
}

\newcommand{\saveandshowplot}[1]{
  \begin{center}
  \includegraphics[width=0.85\textwidth]{#1}
  \end{center}
}

\newtheorem{lemma}{Lemma}

\AtBeginEnvironment{tcolorbox}{\scriptsize}

\begin{document}

\maketitle

\section{The efficiency of different iterative methods for solving a linear system}
\begin{enumerate}[(a)] 
\item 
Following are the implementation for the four methods:\\
\textbf{Jacobi:}
\begin{tcolorbox}
\importandtypeset{part_1_jacobi}
\end{tcolorbox}
\textbf{Gauss-Seidel:}
\begin{tcolorbox}
\importandtypeset{part_1_gauss_seidel}
\end{tcolorbox}
\textbf{Steepest Descent:}
\begin{tcolorbox}
\importandtypeset{part_1_sd}
\end{tcolorbox}
\textbf{Conjugate Gradient}
\begin{tcolorbox}
\importandtypeset{part_1_cg}
\end{tcolorbox}
\item Following are the system and parameters definition, methods calls, residual vector norm and convergence factor plotting:
\begin{scriptsize}
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_1_b}
\end{tcolorbox}
\end{scriptsize}
\end{enumerate}
\section{Convergence properties}
\begin{enumerate}[(a)] 
\item 
\begin{lemma} \label{a}
\[0 < \alpha < \frac{2}{\lambda_{max}} \Rightarrow \rho(I-\alpha A) < 1\]
\end{lemma}
\begin{proof}
$A$ is symmetric positive definite matrix, thus:
\[0 < \lambda_{min} \le \dots \le \lambda_{max}\]
Therefore, we get that:
\[\rho(I-\alpha A) = max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|)\]
We will show the values of $|1-\alpha \lambda_{min}|$ and $|1-\alpha \lambda_{max}|$:

\[ \alpha\lambda_{max}<2 \Rightarrow 1-\alpha\lambda_{max} > -1\]

\[\alpha\lambda_{max}>0 \Rightarrow 1-\alpha\lambda_{max}<1 \]

\[-1 < 1-\alpha \lambda_{max} < 1 \Rightarrow |1-\alpha \lambda_{max}| < 1\]
And,

\[2\frac{\lambda_{min}}{\lambda_{max}} \leq 2 \Rightarrow -1 \leq 1-2\frac{\lambda_{min}}{\lambda_{max}} \]

\[ \alpha < \frac{2}{\lambda_{max}} \Rightarrow \alpha\lambda_{min} < \frac{2\lambda_{min}}{\lambda_{max}} \Rightarrow 1-\alpha\lambda_{min}>\frac{2\lambda_{min}}{\lambda_{max}}\]

\[-1 \leq 1-2 \frac{\lambda_{min} }{\lambda_{max} } < 1 - \alpha \lambda_{min} < 1 \Rightarrow |1-\alpha \lambda_{min}| < 1\]

\[\alpha\lambda_{min}>0 \Rightarrow 1-\alpha\lambda_{min}<1 \]

Thus,
\[max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|) < 1\]
So we get that:
\[\rho(I-\alpha A) < 1\]
\end{proof}
In our case:
\[\alpha = \frac{1}{||A||}\]
We know that for any induced norm:
\[||A|| > \rho(A) = \lambda_{max}\]
Thus,
\[\frac{1}{||A||} < \frac{1}{\lambda_{max}} < \frac{2}{\lambda_{max}}\]
Therefore, by \textbf{Lemma 1} we get that
\[\rho(I-\alpha A) < 1 \]
And the method converges.
\item In the case $A$ is indefinite, we a negative eigenvalue, therefore:
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}|\]
$\lambda_{min} < 0$, by definition, therefore
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}| > 1\]
\item
\begin{enumerate}[label=(\roman*)]
\item
\[f(\textbf{x}) = \frac{1}{2}||\textbf{x}^*-\textbf{x}||^2_A\]
\begin{align*}
f(\mathbf{x}^{(k)}) &=\frac{1}{2}||\mathbf{x}^*-\mathbf{x^{(k)}}||^2_A\\
&= \frac{1}{2}||\mathbf{e}^{(k)}||^2_A\\
&= \frac{1}{2}((\mathbf{e}^{(k)})^TA\mathbf{e}^{(k)})\\
&= \frac{1}{2}\langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle
\end{align*}
\begin{align*}
f(\mathbf{x}^{(k+1)}) &= f(\mathbf{x}^{(k)}) + \alpha \mathbf{r}^{(k)}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \alpha \langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle +\frac{1}{2}\alpha^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle^2} && *\alpha = \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
\end{align*}
We get that:
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\]
$A$ is symmetric positive definite matrix, thus
\[\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}>0\]
and therefore,
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} <  f(\mathbf{x}^{(k)})\]
\item From previous section:
\[f(\mathbf{x}^{(k+1)}) = C^{(k)} f(\mathbf{x}^{(k)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} \]
thus,
\[C^{(k)} = 1- \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle f(\mathbf{x}^{(k)})}\]
finally,
\[C^{(k)} = 1- \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle \langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle}\]
\item First, we will proof that $C^{(k)} \leq 1-\frac{\lambda_{min}} {\lambda_{max}}$

\[1- \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle \langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle}\ \leq 1-\frac{\lambda_{min}} {\lambda_{max}}\]

\[
\frac{\lambda_{min}} {\lambda_{max}} \leq \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle \langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle} = \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A \mathbf{r}^{(k)}\rangle} \cdot \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}{\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle} \leq \lambda_{min} \cdot \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}{\langle A^{-1}\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}\]

\[
=\lambda_{min} \cdot \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}{\langle \mathbf{r}^{(k)},
A^{-1}\mathbf{r}^{(k)}\rangle}
\]

For each $\lambda_{i}$ in A we get that the corresponding eigenvalue in $A^{-1}$  is $\frac{1}{\lambda_{i}}$. Thus we get that the minimum eigenvalue for $A^{-1} = \frac{1}{\lambda_{max}}$ where $\lambda_{max}$ is with respect to A. Therefore:

\[
=\lambda_{min} \cdot \frac{\langle\mathbf{r}^{(k)}, \mathbf{r}^{(k)}\rangle}{\langle \mathbf{r}^{(k)},
A^{-1}\mathbf{r}^{(k)}\rangle} \leq \frac{\lambda_{min}}{\lambda_{max}}
\]

Now, we will show that $1-\frac{\lambda_{min}} {\lambda_{max}} < 1\\$
A is symmetric positive definite, Thus all eigenvalues are larger than 0 and clearly $0 < \frac{\lambda_{min}}{\lambda_{max}} \leq 1$
\[-1 < \frac{\lambda_{min}}{\lambda_{max}}-1\]
\[1-\frac{\lambda_{min}}{\lambda_{max}} < 1\]

\item

\[
f(x^{k})=f(x^{0})\prod_{i=1}^{k} C^{(i)}
\]
Since $C^{(k)}<1$ we get that $\lim_{k \to \infty} f(\mathbf{x}^{(k)}) = 0$
\[
f(x^{(k)}) = |b-A \mathbf{x}^{(k)}| = |A\mathbf{x}^{*}-A \mathbf{x}^{(k)}|=|A(\mathbf{x}^{*}-\mathbf{x}^{(k)})|
\]
Since A is fully ranked $|A(\mathbf{x}^{*}-\mathbf{x}^{(k)})|=0$ only if $\mathbf{x}^{(k)}=\mathbf{x}^{*}$, hence $\lim_{k \to \infty} \mathbf{x}^{(k)} = \mathbf{x}^{*}$

\end{enumerate}
\end{enumerate}
\section{GMRES(1) method}
\begin{enumerate}[(a)]
\item
\[||\mathbf{r}^{(k+1)}||_2 =||\mathbf{b}-A\mathbf{x}^{(k+1)}||_2 \]
we define the following scalar function $g(\alpha)$:
\begin{align*}
g(\alpha) &\triangleq  f(\mathbf{x}^{(k)} + \alpha \mathbf{r}^{(k)})\\
&= \frac{1}{2}||\mathbf{b}-A\mathbf{x}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}||\mathbf{r}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-\alpha(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+\frac{1}{2}\alpha^2(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}
\end{align*}
And the minimization of $g$ with respect to $\alpha$ is done by:
\[g'(\alpha)  =-(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+ \alpha  (A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)} = 0\]
\[\Rightarrow \alpha_{opt} = \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}  =  \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(\mathbf{r}^{(k)})^TA^TA\mathbf{r}^{(k)}} \]

\item (non-mandatory)
\item a:
\begin{tcolorbox}[%
    enhanced,
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_3}
\end{tcolorbox}
\item At each step we choose $\alpha^{(k)}$ that minimizes the expression $||r^{(k+1)}||_2$. At the "worst case", when we couldn't decrease the norm of the residuals, $\alpha^{(k)}$ can be chosen to be 0, otherwise $\alpha^{(k)}$ will be chosen such that $||r^{(k+1)}||_2 < ||r^{(k+1)}||$, thus the graph is monotone.
\item

we define the following function $g(\alpha)$:
\begin{align*}
g(\alpha) &\triangleq  f(\mathbf{x}^{(k)} + R^{(k)}\vec{\alpha}^{(k)})\\
&= \frac{1}{2}||\mathbf{b}-A(\mathbf{x}^{(k)} + R^{(k)}\vec{\alpha}^{(k)})||_2\\
&= \frac{1}{2}||\mathbf{b}-A\mathbf{x}^{(k)}-AR^{(k)}\vec{\alpha}^{(k)}||_2\\
&= \frac{1}{2}||\mathbf{r}^{(k)}-AR^{(k)}\vec{\alpha}^{(k)}||_2\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-(\mathbf{r}^{(k)})^TAR^{(k)}\vec{\alpha}^{(k)}+\frac{1}{2}(AR^{(k)}\vec{\alpha}^{(k)})^T(AR^{(k)}\vec{\alpha}^{(k)})\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-(\mathbf{r}^{(k)})^TAR^{(k)}\vec{\alpha}^{(k)}+\frac{1}{2}(\vec{\alpha}^{(k)})^{T}(AR^{(k)})^T(AR^{(k)}\vec{\alpha}^{(k)})\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-(\mathbf{r}^{(k)})^TAR^{(k)}\vec{\alpha}^{(k)}+\frac{1}{2}(\vec{\alpha}^{(k)})^{T}(R^{(k)})^{T}A^{T}AR^{(k)}\vec{\alpha}^{(k)}
\end{align*}

Now, we derive the function by $\vec{\alpha}^{(k)}$ in order to find $\vec{\alpha}^{(k)}$ that minimize the expression. we get that

\[
(\mathbf{r}^{(k)})^TAR^{(k)} = \frac{1}{2}(\vec{\alpha}^{(k)})^{T}( ((R^{(k)})^{T}A^{T}AR^{(k)})^{T}+(R^{(k)})^{T}A^{T}AR^{(k)} )
\]

\[
(\vec{\alpha}^{(k)})^{T} = 2(\mathbf{r}^{(k)})^TAR^{(k)}( ((R^{(k)})^{T}A^{T}AR^{(k)})^{T}+(R^{(k)})^{T}A^{T}AR^{(k)} )^{-1}
\]

\end{enumerate}
\section{Convexity}
\begin{enumerate}[(a)] 
\item \begin{enumerate}[label=\roman*.]
\item $e^{ax}$ is convex:
\[(e^{ax})'' = a^2e^{ax} \ge 0 \quad \forall x\]
\item $-log(x)$ is convex:
\[(-log(x))'' = \frac{1}{x^2} > 0 \quad \forall x > 0\]
\item $log(x)$ is concave:
\[(log(x))'' = -\frac{1}{x^2} < 0 \quad \forall x > 0\]
\item $|x|^a, \quad a \ge 1$ is convex:
\begin{align*}
f(\alpha x+(1-\alpha)y) &= |\alpha x+(1-\alpha)y|^a\\
&\le (|\alpha x|+|(1-\alpha)y|)^a && \text{*triangle inequality}\\
&= (\alpha |x|+(1-\alpha)|y|)^a \\
&\le \alpha |x|^a+(1-\alpha)|y|^a && *\alpha \le 1\\
&= \alpha f(x)+(1-\alpha)f(y)
\end{align*}
\item $x^3$ is none of those:\\
Not convex: \\
for $x=-1,y=0,\alpha=0.5$:
\[f(\alpha x+(1-\alpha)y) = f(0.5(-1)+(1-0.5)0) = f(-0.5) = (-0.5)^3 = -0.125\]
\[\alpha f(x)+(1-\alpha)f(y) = 0.5 f(-1)+(1-0.5)f(0) = 0.5(-1)^3 = -0.5\]
we get that
\[f(\alpha x+(1-\alpha)y) > \alpha f(x)+(1-\alpha)f(y)\]
Not concave: \\
for $x=0,y=1,\alpha=0.5$:
\[f(\alpha x+(1-\alpha)y) = f(0.5(0)+(1-0.5)1) = f(0.5) = (0.5)^3 = 0.125\]
\[\alpha f(x)+(1-\alpha)f(y) = 0.5 f(0)+(1-0.5)f(1) = 0.5(1)^3 = 0.5\]
we get that
\[f(\alpha x+(1-\alpha)y) < \alpha f(x)+(1-\alpha)f(y)\]
\end{enumerate}
\item Let
\[f(\mathbf{x}) = \mathbf{x}^TA\mathbf{x} + \mathbf{b}^T\mathbf{x} + \mathbf{c}\]
computing the Hessian of $f(\mathbf{x})$
\[\nabla f(\mathbf{x}) = 2A\mathbf{x} + \mathbf{b}\]
\[\nabla^2 f(\mathbf{x}) = J(\nabla f(\mathbf{x}) )= 2A\]
$ f(\mathbf{x})$ is a convex function over a convex region $\Omega$ if and only if $2A\succeq 0$, is positive semi definite.
\item Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable in a convex domain $\Omega$. We'll show the following:
\[f \quad\text{is convex}\Leftrightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
$\Rightarrow$:\\
$f$ is convex. Then, according to the fundamental definition of convex functions, the following inequality condition must be satisfied:
\begin{align*}
&f(\alpha x + (1-\alpha)y) \le \alpha f(x)+ (1-\alpha)f(y)&&\forall x,y \in \Omega \land \alpha \in [0,1]\\
&\Rightarrow f(x+\alpha (y-x)) \le f(x)+ \alpha(f(y)-f(x))\\
&\Rightarrow f(x+\alpha (y-x))-f(x) \le \alpha(f(y)-f(x))\\
&\Rightarrow \frac{f(x+\alpha (y-x))-f(x) }{\alpha}\le f(y)-f(x)\\
&\Rightarrow f(y)\ge f(x)+\frac{f(x+\alpha (y-x))-f(x) }{\alpha} \\
\end{align*}
Now, let
\[g(\alpha)=f(x+\alpha (y-x))\]
therefore
\[f(y)\ge f(x)+\frac{g(\alpha)-g(0) }{\alpha}\quad * g(0) = f(x)\]
Now taking the limit as $\alpha \rightarrow 0$ we get
\begin{align*}
&f(y)\ge f(x)+\lim_{\alpha \to 0}\frac{g(\alpha)-g(0) }{\alpha}\\
&\Rightarrow f(y)\ge f(x)+g'(0)
\end{align*}
In order to find $g'(0)$, we'll compute the more general $g'(t)$:
\[g'(t) = \nabla_xf(x+t(y-x))^T(y-x)\]
assigning $t=0$, we get
\[g'(0) =\nabla_xf(x)^T(y-x) \]
finally, by substituting $g'(0)$ we get
\[f(y)\ge f(x)+\nabla_xf(x)^T(y-x)\] 
therefore
\[f \quad\text{is convex}\Rightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
$\Leftarrow$:\\
Let
\[ f(y) > f(x)+\nabla_x f(x)^T(y-x)\quad,\forall x,y \in \Omega\]
Now, consider
\[z= \alpha x+(1-\alpha)y\quad \forall \alpha \in [0,1]\]
Notice that, since $\Omega$ is a convex domain, $z \in \Omega$. Therefore:
\begin{equation}
f(x) > f(z)+\nabla_z f(z)^T(x-z)
\end{equation}
\begin{equation}
f(y) > f(z)+\nabla_z f(z)^T(y-z)
\end{equation}
Now multiplying the inequalities in Equations (1) and (2) with $\alpha$ and $(1-\alpha)$ respectively and adding the results we get:
\[\alpha f(x) + (1-\alpha)f(y) > f(z)+\nabla_z f(z)^T(\alpha x+(1-\alpha)y-z)\]
by substituting $z= \alpha x+(1-\alpha)y$, we get
\begin{align*}
&\alpha f(x) + (1-\alpha)f(y) > f(\alpha x+(1-\alpha)y)+\nabla_z f(z)^T(\alpha x+(1-\alpha)y-\alpha x+(1-\alpha)y)\\
&\Rightarrow \alpha f(x) + (1-\alpha)f(y) > f(\alpha x+(1-\alpha)y)
\end{align*}
Observe that this is exactly the inequality that $f(x)$ must satisfy in order to be considered as a convex function, hence
\[f \quad\text{is convex}\Leftarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \] 
and combining the 2 sides, we showed that
\[f \quad\text{is convex}\Leftrightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
\end{enumerate}
\section{Non Linear Optimization}
\begin{enumerate}[(a)] 
\item 
Following is the implementation for the function that, given data matrix $X$ and labels, computes the logistic
regression objective, its gradient, and its Hessian matrix:\\
\begin{tcolorbox}
\importandtypeset{part_5_a}
\end{tcolorbox}
\item Implementation of Gradient and Jacobian verification:
\begin{tcolorbox}
\importandtypeset{part_5_b}
\end{tcolorbox}
Testing our gradient and hessiam implementation:
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_5_b_run}
\end{tcolorbox}
\item Implementation of Gradient Descent, and exact Newton methods:
\begin{tcolorbox}
\importandtypeset{part_5_c}
\end{tcolorbox}
Running over the MNIST data sets for the digits 0,1 and 8,9 and shwing the results:
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_5_c_run1}
\end{tcolorbox}
\end{enumerate}
\end{document}
