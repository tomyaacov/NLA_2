\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Home assignment 2}
\author{Numerical Optimization and its Applications - Spring 2019\\Gil Ben Shalom, 301908877\\Tom Yaacov, 305578239}
\date{\today}

\usepackage[shortlabels]{enumitem}
\usepackage{pythontex}
\usepackage[most]{tcolorbox}
\usepackage{amsmath,amsthm,amssymb}

\newcommand{\importandtypeset}[1]{
  %\pyc{print(12);add_to_path('py_files'); from #1 import *; pytex.add_dependencies(os.path.join('py_files', '#1.py'))}%
  \inputpygments{python}{py_files/#1.py}%
  \pyc{from py_files import #1}
}

\newcommand{\saveandshowplot}[1]{
  \begin{center}
  \includegraphics[width=0.85\textwidth]{#1}
  \end{center}
}

\newtheorem{lemma}{Lemma}

\begin{document}

\maketitle

\section{The efficiency of different iterative methods for solving a linear system}
\begin{enumerate}[(a)] 
\item 
Following are the implementation for the four methods:\\
\textbf{Jacobi:}
\begin{tcolorbox}
\importandtypeset{part_1_jacobi}
\end{tcolorbox}
\textbf{Gauss-Seidel:}
\begin{tcolorbox}
\importandtypeset{part_1_gauss_seidel}
\end{tcolorbox}
\textbf{Steepest Descent:}
\begin{tcolorbox}
\importandtypeset{part_1_sd}
\end{tcolorbox}
\textbf{Conjugate Gradient}
\begin{tcolorbox}
\importandtypeset{part_1_cg}
\end{tcolorbox}
\item Following are the system and parameters definition, methods calls, residual vector norm and convergence factor plotting:
\begin{scriptsize}
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_1_b}
\end{tcolorbox}
\end{scriptsize}
\end{enumerate}
\section{Convergence properties}
\begin{enumerate}[(a)] 
\item 
\begin{lemma} \label{a}
\[0 < \alpha < \frac{2}{\lambda_{max}} \Rightarrow \rho(I-\alpha A) < 1\]
\end{lemma}
\begin{proof}
$A$ is symmetric positive definite matrix, thus:
\[0 < \lambda_{min} \le \dots \le \lambda_{max}\]
therefore, we get that:
\[\rho(I-\alpha A) = max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|)\]
$|1-\alpha \lambda_{max}|$, then we get that:
\[-1 < 1-\alpha \lambda_{max} < 1 \Rightarrow |1-\alpha \lambda_{max}| < 1\] 
And,
\[-1 < 1-2 \frac{\lambda_{min} }{\lambda_{max} } < 1 - \alpha \lambda_{min} < 1 \Rightarrow |1-\alpha \lambda_{min}| < 1\] 
thus,
\[max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|) < 1\]
so we get that:
\[\rho(I-\alpha A) < 1\]
\end{proof}
In our case:
\[\alpha = \frac{1}{||A||}\]
we know that for any induced norm:
\[||A|| > \rho(A) = \lambda_{max}\]
thus,
\[\frac{1}{||A||} < \frac{1}{\lambda_{max}} < \frac{2}{\lambda_{max}}\]
therefore, by \textbf{Lemma 1} we get that
\[\rho(I-\alpha A) \le 1 \]
and the method converges.
\item In the case $A$ is indefinite, we a negative eigenvalue, therefore:
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}|\]
$\lambda_{min} < 0$, by definition, therefore
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}| > 1\]
\item 
\begin{enumerate}[label=(\roman*)]
\item 
\[f(\textbf{x}) = \frac{1}{2}||\textbf{x}^*-\textbf{x}||^2_A\]
\begin{align*}
f(\mathbf{x}^{(k)}) &=\frac{1}{2}||\mathbf{x}^*-\mathbf{x^{(k)}}||^2_A\\
&= \frac{1}{2}||\mathbf{e}^{(k)}||^2_A\\
&= \frac{1}{2}((\mathbf{e}^{(k)})^TA\mathbf{e}^{(k)})\\
&= \frac{1}{2}\langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle
\end{align*}
\begin{align*}
f(\mathbf{x}^{(k+1)}) &= f(\mathbf{x}^{(k)}) + \alpha \mathbf{r}^{(k)}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \alpha \langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle +\frac{1}{2}\alpha^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle^2} && *\alpha = \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
\end{align*}
We get that:
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\]
$A$ is symmetric positive definite matrix, thus
\[\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}>0\]
and therefore,
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} <  f(\mathbf{x}^{(k)})\]
\item From previous section:
\[f(\mathbf{x}^{(k+1)}) = C^{(k)} f(\mathbf{x}^{(k)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} \]
thus,
\[C^{(k)} = 1- \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle f(\mathbf{x}^{(k)})}\]
finally,
\[C^{(k)} = 1- \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle \langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle}\]
\item t
\item t
\end{enumerate}
\end{enumerate}
\section{GMRES(1) method}
\begin{enumerate}[(a)] 
\item 
\[||\mathbf{r}^{(k+1)}||_2 =||\mathbf{b}-A\mathbf{x}^{(k+1)}||_2 \]
we define the following scalar function $g(\alpha)$:
\begin{align*}
g(\alpha) &\triangleq  f(\mathbf{x}^{(k)}) + \alpha \mathbf{r}^{(k)}\\
&= \frac{1}{2}||\mathbf{b}-A\mathbf{x}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}||\mathbf{r}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-\alpha(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+\frac{1}{2}\alpha^2(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}
\end{align*}
And the minimization of $g$ with respect to $\alpha$ is done by:
\[g'(\alpha)  =-(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+ \alpha  (A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)} = 0\]
\[\Rightarrow \alpha_{opt} = \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}  =  \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(\mathbf{r}^{(k)})^TA^TA\mathbf{r}^{(k)}} \]

\item (non-mandatory)
\item a:
\begin{scriptsize}
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_3}
\end{tcolorbox}
\end{scriptsize}
\item t
\item t
\end{enumerate}
\section{Convexity}
\begin{enumerate}[(a)] 
\item \begin{enumerate}[label=\roman*.]
\item $e^{ax}$ is convex:
\[(e^{ax})'' = a^2e^{ax} \ge 0 \quad \forall x\]
\item $-log(x)$ is convex:
\[(-log(x))'' = \frac{1}{x^2} > 0 \quad \forall x > 0\]
\item $log(x)$ is concave:
\[(log(x))'' = -\frac{1}{x^2} < 0 \quad \forall x > 0\]
\item $|x|^a, \quad a \ge 1$ is convex:
\begin{align*}
f(\alpha x+(1-\alpha)y) &= |\alpha x+(1-\alpha)y|^a\\
&\le 
\end{align*}
\item t
\end{enumerate}
\item t
\item t
\end{enumerate}
\section{Non Linear Optimization}
\begin{enumerate}[(a)] 
\item t
\item t
\item t
\end{enumerate}
\end{document}
