\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Home assignment 2}
\author{Numerical Optimization and its Applications - Spring 2019\\Gil Ben Shalom, 301908877\\Tom Yaacov, 305578239}
\date{\today}

\usepackage[shortlabels]{enumitem}
\usepackage{pythontex}
\usepackage[most]{tcolorbox}
\usepackage{amsmath,amsthm,amssymb}

\newcommand{\importandtypeset}[1]{
  %\pyc{print(12);add_to_path('py_files'); from #1 import *; pytex.add_dependencies(os.path.join('py_files', '#1.py'))}%
  \inputpygments{python}{py_files/#1.py}%
  \pyc{from py_files import #1}
}

\newcommand{\saveandshowplot}[1]{
  \begin{center}
  \includegraphics[width=0.85\textwidth]{#1}
  \end{center}
}

\newtheorem{lemma}{Lemma}

\begin{document}

\maketitle

\section{The efficiency of different iterative methods for solving a linear system}
\begin{enumerate}[(a)] 
\item 
Following are the implementation for the four methods:\\
\textbf{Jacobi:}
\begin{tcolorbox}
\importandtypeset{part_1_jacobi}
\end{tcolorbox}
\textbf{Gauss-Seidel:}
\begin{tcolorbox}
\importandtypeset{part_1_gauss_seidel}
\end{tcolorbox}
\textbf{Steepest Descent:}
\begin{tcolorbox}
\importandtypeset{part_1_sd}
\end{tcolorbox}
\textbf{Conjugate Gradient}
\begin{tcolorbox}
\importandtypeset{part_1_cg}
\end{tcolorbox}
\item Following are the system and parameters definition, methods calls, residual vector norm and convergence factor plotting:
\begin{scriptsize}
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_1_b}
\end{tcolorbox}
\end{scriptsize}
\end{enumerate}
\section{Convergence properties}
\begin{enumerate}[(a)] 
\item 
\begin{lemma} \label{a}
\[0 < \alpha < \frac{2}{\lambda_{max}} \Rightarrow \rho(I-\alpha A) < 1\]
\end{lemma}
\begin{proof}
$A$ is symmetric positive definite matrix, thus:
\[0 < \lambda_{min} \le \dots \le \lambda_{max}\]
therefore, we get that:
\[\rho(I-\alpha A) = max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|)\]
$|1-\alpha \lambda_{max}|$, then we get that:
\[-1 < 1-\alpha \lambda_{max} < 1 \Rightarrow |1-\alpha \lambda_{max}| < 1\] 
And,
\[-1 < 1-2 \frac{\lambda_{min} }{\lambda_{max} } < 1 - \alpha \lambda_{min} < 1 \Rightarrow |1-\alpha \lambda_{min}| < 1\] 
thus,
\[max(|1-\alpha \lambda_{min}|, |1-\alpha \lambda_{max}|) < 1\]
so we get that:
\[\rho(I-\alpha A) < 1\]
\end{proof}
In our case:
\[\alpha = \frac{1}{||A||}\]
we know that for any induced norm:
\[||A|| > \rho(A) = \lambda_{max}\]
thus,
\[\frac{1}{||A||} < \frac{1}{\lambda_{max}} < \frac{2}{\lambda_{max}}\]
therefore, by \textbf{Lemma 1} we get that
\[\rho(I-\alpha A) \le 1 \]
and the method converges.
\item In the case $A$ is indefinite, we a negative eigenvalue, therefore:
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}|\]
$\lambda_{min} < 0$, by definition, therefore
\[\rho(I-\alpha A)\ge |1-\alpha \lambda_{min}| > 1\]
\item 
\begin{enumerate}[label=(\roman*)]
\item 
\[f(\textbf{x}) = \frac{1}{2}||\textbf{x}^*-\textbf{x}||^2_A\]
\begin{align*}
f(\mathbf{x}^{(k)}) &=\frac{1}{2}||\mathbf{x}^*-\mathbf{x^{(k)}}||^2_A\\
&= \frac{1}{2}||\mathbf{e}^{(k)}||^2_A\\
&= \frac{1}{2}((\mathbf{e}^{(k)})^TA\mathbf{e}^{(k)})\\
&= \frac{1}{2}\langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle
\end{align*}
\begin{align*}
f(\mathbf{x}^{(k+1)}) &= f(\mathbf{x}^{(k)}) + \alpha \mathbf{r}^{(k)}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \alpha \langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle +\frac{1}{2}\alpha^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle^2} && *\alpha = \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} + \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= \frac{1}{2}\langle\mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
&= f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\\
\end{align*}
We get that:
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}\]
$A$ is symmetric positive definite matrix, thus
\[\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle}>0\]
and therefore,
\[f(\mathbf{x}^{(k+1)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} <  f(\mathbf{x}^{(k)})\]
\item From previous section:
\[f(\mathbf{x}^{(k+1)}) = C^{(k)} f(\mathbf{x}^{(k)}) = f(\mathbf{x}^{(k)}) - \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle} \]
thus,
\[C^{(k)} = 1- \frac{1}{2}\frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle f(\mathbf{x}^{(k)})}\]
finally,
\[C^{(k)} = 1- \frac{\langle\mathbf{r}^{(k)}, A\mathbf{e}^{(k)}\rangle^2}{\langle\mathbf{r}^{(k)}, A\mathbf{r}^{(k)}\rangle \langle \mathbf{e}^{(k)}, A\mathbf{e}^{(k)}\rangle}\]
\item t
\item t
\end{enumerate}
\end{enumerate}
\section{GMRES(1) method}
\begin{enumerate}[(a)] 
\item 
\[||\mathbf{r}^{(k+1)}||_2 =||\mathbf{b}-A\mathbf{x}^{(k+1)}||_2 \]
we define the following scalar function $g(\alpha)$:
\begin{align*}
g(\alpha) &\triangleq  f(\mathbf{x}^{(k)}) + \alpha \mathbf{r}^{(k)}\\
&= \frac{1}{2}||\mathbf{b}-A\mathbf{x}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}||\mathbf{r}^{(k)}-\alpha A\mathbf{r}^{(k)}||_2\\
&= \frac{1}{2}(\mathbf{r}^{(k)})^T\mathbf{r}^{(k)}-\alpha(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+\frac{1}{2}\alpha^2(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}
\end{align*}
And the minimization of $g$ with respect to $\alpha$ is done by:
\[g'(\alpha)  =-(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}+ \alpha  (A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)} = 0\]
\[\Rightarrow \alpha_{opt} = \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(A\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}  =  \frac{(\mathbf{r}^{(k)})^TA\mathbf{r}^{(k)}}{(\mathbf{r}^{(k)})^TA^TA\mathbf{r}^{(k)}} \]

\item (non-mandatory)
\item a:
\begin{scriptsize}
\begin{tcolorbox}[%
    enhanced, 
    breakable,
    frame hidden,
    overlay broken = {
        \draw[line width=0.5mm, black, rounded corners]
        (frame.north west) rectangle (frame.south east);},
    ]{}
\importandtypeset{part_3}
\end{tcolorbox}
\end{scriptsize}
\item t
\item t
\end{enumerate}
\section{Convexity}
\begin{enumerate}[(a)] 
\item \begin{enumerate}[label=\roman*.]
\item $e^{ax}$ is convex:
\[(e^{ax})'' = a^2e^{ax} \ge 0 \quad \forall x\]
\item $-log(x)$ is convex:
\[(-log(x))'' = \frac{1}{x^2} > 0 \quad \forall x > 0\]
\item $log(x)$ is concave:
\[(log(x))'' = -\frac{1}{x^2} < 0 \quad \forall x > 0\]
\item $|x|^a, \quad a \ge 1$ is convex:
\begin{align*}
f(\alpha x+(1-\alpha)y) &= |\alpha x+(1-\alpha)y|^a\\
&\le (|\alpha x|+|(1-\alpha)y|)^a && \text{*triangle inequality}\\
&= (\alpha |x|+(1-\alpha)|y|)^a \\
&\le \alpha |x|^a+(1-\alpha)|y|^a && *\alpha \le 1\\
&= \alpha f(x)+(1-\alpha)f(y)
\end{align*}
\item $x^3$ is none of those:\\
Not convex: \\
for $x=-1,y=0,\alpha=0.5$:
\[f(\alpha x+(1-\alpha)y) = f(0.5(-1)+(1-0.5)0) = f(-0.5) = (-0.5)^3 = -0.125\]
\[\alpha f(x)+(1-\alpha)f(y) = 0.5 f(-1)+(1-0.5)f(0) = 0.5(-1)^3 = -0.5\]
we get that
\[f(\alpha x+(1-\alpha)y) > \alpha f(x)+(1-\alpha)f(y)\]
Not concave: \\
for $x=0,y=1,\alpha=0.5$:
\[f(\alpha x+(1-\alpha)y) = f(0.5(0)+(1-0.5)1) = f(0.5) = (0.5)^3 = 0.125\]
\[\alpha f(x)+(1-\alpha)f(y) = 0.5 f(0)+(1-0.5)f(1) = 0.5(1)^3 = 0.5\]
we get that
\[f(\alpha x+(1-\alpha)y) < \alpha f(x)+(1-\alpha)f(y)\]
\end{enumerate}
\item Let
\[f(\mathbf{x}) = \mathbf{x}^TA\mathbf{x} + \mathbf{b}^T\mathbf{x} + \mathbf{c}\]
computing the Hessian of $f(\mathbf{x})$
\[\nabla f(\mathbf{x}) = 2A\mathbf{x} + \mathbf{b}\]
\[\nabla^2 f(\mathbf{x}) = J(\nabla f(\mathbf{x}) )= 2A\]
$ f(\mathbf{x})$ is a convex function over a convex region $\Omega$ if and only if $2A\succeq 0$, is positive semi definite.
\item Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable in a convex domain $\Omega$. We'll show the following:
\[f \quad\text{is convex}\Leftrightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
$\Rightarrow$:\\
$f$ is convex. Then, according to the fundamental definition of convex functions, the following inequality condition must be satisfied:
\begin{align*}
&f(\alpha x + (1-\alpha)y) \le \alpha f(x)+ (1-\alpha)f(y)&&\forall x,y \in \Omega \land \alpha \in [0,1]\\
&\Rightarrow f(x+\alpha (y-x)) \le f(x)+ \alpha(f(y)-f(x))\\
&\Rightarrow f(x+\alpha (y-x))-f(x) \le \alpha(f(y)-f(x))\\
&\Rightarrow \frac{f(x+\alpha (y-x))-f(x) }{\alpha}\le f(y)-f(x)\\
&\Rightarrow f(y)\ge f(x)+\frac{f(x+\alpha (y-x))-f(x) }{\alpha} \\
\end{align*}
Now, let
\[g(\alpha)=f(x+\alpha (y-x))\]
therefore
\[f(y)\ge f(x)+\frac{g(\alpha)-g(0) }{\alpha}\quad * g(0) = f(x)\]
Now taking the limit as $\alpha \rightarrow 0$ we get
\begin{align*}
&f(y)\ge f(x)+\lim_{\alpha \to 0}\frac{g(\alpha)-g(0) }{\alpha}\\
&\Rightarrow f(y)\ge f(x)+g'(0)
\end{align*}
In order to find $g'(0)$, we'll compute the more general $g'(t)$:
\[g'(t) = \nabla_xf(x+t(y-x))^T(y-x)\]
assigning $t=0$, we get
\[g'(0) =\nabla_xf(x)^T(y-x) \]
finally, by substituting $g'(0)$ we get
\[f(y)\ge f(x)+\nabla_xf(x)^T(y-x)\] 
therefore
\[f \quad\text{is convex}\Rightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
$\Leftarrow$:\\
Let
\[ f(y) > f(x)+\nabla_x f(x)^T(y-x)\quad,\forall x,y \in \Omega\]
Now, consider
\[z= \alpha x+(1-\alpha)y\quad \forall \alpha \in [0,1]\]
Notice that, since $\Omega$ is a convex domain, $z \in \Omega$. Therefore:
\begin{equation}
f(x) > f(z)+\nabla_z f(z)^T(x-z)
\end{equation}
\begin{equation}
f(y) > f(z)+\nabla_z f(z)^T(y-z)
\end{equation}
Now multiplying the inequalities in Equations (1) and (2) with $\alpha$ and $(1-\alpha)$ respectively and adding the results we get:
\[\alpha f(x) + (1-\alpha)f(y) > f(z)+\nabla_z f(z)^T(\alpha x+(1-\alpha)y-z)\]
by substituting $z= \alpha x+(1-\alpha)y$, we get
\begin{align*}
&\alpha f(x) + (1-\alpha)f(y) > f(\alpha x+(1-\alpha)y)+\nabla_z f(z)^T(\alpha x+(1-\alpha)y-\alpha x+(1-\alpha)y)\\
&\Rightarrow \alpha f(x) + (1-\alpha)f(y) > f(\alpha x+(1-\alpha)y)
\end{align*}
Observe that this is exactly the inequality that $f(x)$ must satisfy in order to be considered as a convex function, hence
\[f \quad\text{is convex}\Leftarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \] 
and combining the 2 sides, we showed that
\[f \quad\text{is convex}\Leftrightarrow f(y) > f(x)+\nabla f(x)^T(y-x)\quad,\forall x,y \in \Omega \]
\end{enumerate}
\section{Non Linear Optimization}
\begin{enumerate}[(a)] 
\item t
\item t
\item t
\end{enumerate}
\end{document}
